#==============================================================================
# General migration process options
#==============================================================================
[migrate]

# Print debugging output (set logging level to DEBUG instead of default INFO
# level).
debug = False

# The path of a logging configuration file.
logging_config = configs/logging_config.yaml

# Forward messages from stdout to log.
# Default value is True
forward_stdout = True

# Migration scenario to be used
# Sample scenarios:
#   - `scenario/cold_migrate.yaml`: cold migration scenario
#   - `scenario/live_migrate.yaml`: live migration scenario
#   - `scenario/migrate_resources.yaml`: scenario which only migrates resources
#   - `scenario/migrate_vms.yaml`: scenario which only migrates VMs, assumes
#     resources were migrated using `scenario/migrate_resources.yaml` scenario
scenario = scenario/migrate.yaml

# Path to YAML file which holds relationships between code and actions defined
# in `scenario`
tasks_mapping = scenario/tasks.yaml

# Keep the order of network interfaces of instances.
# Default value is True.
keep_network_interfaces_order = True

# Enable or Disable capability to keep  user passwords value from keystone.
# Default value is False.
# Values:
#   False - new passwords will be generated for users after migration.
#   True - same passwords will be after migration(scenario needs testing)
keep_user_passwords = False

# Path for ssh private key file e.g. ~/.ssh/id_rsa. Used for both Clouds.
key_filename = <path to ssh private key>

# Preserve private IPs for instances migrated on destination cloud same as in
# source cloud.
keep_ip = yes

ext_net_map = configs/ext_net_map.yaml

# Keep same Floating IPs for instances migrated on destination cloud as in
# source cloud.
# Values:
#  True - to keep floating IPs. In that case after migration floating IPs will
#         be transferred to the DST.
#  False - new IPs will be attached on the DST if customer provides different
#          external ip pool.
keep_floatingip = yes

# Speed limit for glance to glance transfer.
# Values: [off|Nb|Nkb|Nmb|Ngb], where:
# off - Speed limit is disabled (Default value)
# N - int value (f.e. 1, 100, 1024 etc.)
# b/kb/mb/gb - a multiple of the unit byte for glance image info transfer
speed_limit = off

# Overwrite password for existing users on destination Cloud.
# Values:
# True - overwrite password for existing user and send them notification.
# False - do not overwrite password.
# IMPORTANT: currently functions incorrectly!
overwrite_user_passwords = False

# Enable or disable tenant quotas Migration.
migrate_quotas = False

# Enable or disable tenant quotas Migration.
migrate_users = True

#Enable recalculating usage of quotas when we change user of instance
keep_usage_quotas_inst = True

# Skip key pairs that belong to deleted tenant or to deleted user on SRC.
# If it's set to False - the key pairs will be assigned to the admin.
skip_orphaned_keypairs = True

# Direct data transmission between (src and dst) compute nodes via ssh tunnel.
# Values:
#  yes - Used when network connectivity between src and dst compute nodes
#        exist.
#  no - Used when no connectivity b/n computes, then connection via external
#       network.
direct_transfer = True

# CloudFerry tool provides filtering per-tenant, per-instance, per-image.
# This filter will allow to migrate specified tenants, instances, images only.
# Resources not specified in filter will be migrated by default(all for all
# tenants).
# See `configs/filter.yaml` for more
filter_path = configs/filter.yaml

# Migrate or not migrate lbaas settings for Neutron.
# By lbaas settings means: pools, members, health monitors, VIPs and their
# relationships.
keep_lbaas = no

# Size of one chunk to transfer via SSH in Mb.
ssh_chunk_size = 100

# Number x API retries.
# Note: High number may considerably slow down migration process, but ensures
# retry.
retry = 5

# Migrate or not migrate external networks for Neutron.
migrate_extnets = True

# Time in seconds between Openstack API retries.
# Note: High number may considerably slow down migration process.
time_wait = 5

# Configuration file to be used for grouping instances based on user-defined
# criteria.
group_file_path = configs/grouping_result.yaml

# Migrate user quotas. If it set in "false" only tenant quotas will be migrated.
# Use this in case when OpenStack does not support user quotas (e.g. Grizzly)
migrate_user_quotas = False

# Backend used for doing live migration inside a cloud. Used by evacuation chain.
# "nova" by default. Other possible option - "cobalt". Raises error in case
# set to any other value.
incloud_live_migration = nova


# Uses low-level DB requests if set to True, may be incompatible with more
# recent versions of Keystone. Tested on grizzly, icehouse and juno. True by
# default
# optimize_user_role_fetch = [True|False]
optimize_user_role_fetch = True

# Number of times CloudFerry will attempt to connect when connecting to a new
# server via SSH.
ssh_connection_attempts = 3

# Ignore images with size 0 and exclude them from migration process
ignore_empty_images = False

# Don't show ssl warnings
hide_ssl_warnings = True

# Keep affinity/anti-affinity settings (Nova server-groups migration).
# Values:
# True - enabled
# False - disabled (by default)
keep_affinity_settings = True

# Timeout booting of instance
boot_timeout = 300

# Specify SSH cipher to use while copying ephemeral disks, etc...
ssh_cipher = arcfour128

# Allows to choose how ephemeral storage and cinder volumes are copied over
# from source to destination. Possible values: 'rsync' (default), 'scp' and
# 'bbcp'. scp seem to be faster, while rsync is more reliable and bbcp
# but bbcp works in multithreading mode.
copy_backend = rsync

# Calculate md5 checksum for source and copied files and verify them.
copy_with_md5_verification = True

# Availability zone to use for VM provisioning, in case source cloud zones do
# not match destination
default_availability_zone = nova

# Server creation parameter (e.g. server group) override rules file path.
override_rules = configs/override_rules.yaml

# Time in seconds to wait for cinder objects creation. Set to 300 by default.
storage_backend_timeout = 300

# Migrate the whole cloud.
# Values:
# True - enabled. Whole cloud will be migrated despite the filter config file
#                 is specified;
# False - disabled (by default). Migrate resources and instances according to
#         the filter config file.
migrate_whole_cloud = False

# Time to wait for image backed to save an image.
# Value is in seconds.
image_save_timeout = 600

# Filename template to store a dump of a database.
db_dump_filename = dump_{position}_{database}_{time}.sql

# Run mysqldump command on database's host. Otherwise mysqldump will be
# executed on current host.
db_dump_on_db_host = False

#==============================================================================
# Source cloud configuration
#==============================================================================
[src]

# Currently only supported value "os" - OpenStack. Future release may include
# other Clouds. Reserved for future.
type = os

# Keystone's address on service endpoint for authorization purposes (External
# IP)
# `keystone endpoint-list | grep 35357`
auth_url = <src_auth_url>

# Public (external) IP of Controller used for API interaction.
host = <src_api_host>

# Public (external) IP of cloud node used for ssh communication (usually
# Controller node).
ssh_host = <src_ssh_host>

# User to connect via ssh
ssh_user = <src_ssh_user>

# Sudo password to connect via ssh on compute nodes, and run commands with
# root privileges.
ssh_sudo_password = <src_ssh_sudo_password>

# External network CIDR of source Clouds compute nodes. You can specify several
# CIDRs using comma as delimiter, for example:
# ext_cidr = 8.8.0.0/16,4.4.0.0/16
# TODO: consider using bridge name instead
ext_cidr = <src_external_net_cidr>

# Openstack admin user which belongs to admin tenant for access to API.
user = admin

# Password of OpenStack admin user for access to API.
password = admin

# Password of OpenStack admin user for access to API.
tenant = admin

# Region name for Openstack services. `None` by default
region = myregion

# "service_tenant" is the name for Tenant used by openstack services.
# By default CloudFerry takes following value for service tenant name:
# "service". In some cases (e.g Mirantis Openstack) the service tenant name
# can be different. To verify if value is different from default run:
# `keystone tenant-list | grep service`
service_tenant = service

# SSL sertificate file for establish connection to secure OpenStack services
cacert = <sertificate_file>

# Allow to access servers without checking SSL certs
insecure = False


#==============================================================================
# Source cloud MySQL configuration
#==============================================================================
[src_mysql]

# MySQL DB user with write permissions
db_user = root

# Root Password for mysql
# For mysql: grep "password" /etc/mysql/my.cnf.
# For Galera: grep "bind" /etc/mysql/conf.d/wsrep.cnf
db_password = top_secret_password

# Hostname or IP address of MySQL server
# For mysql: grep "bind" /etc/mysql/my.cnf.
# For Galera: grep "bind" /etc/mysql/conf.d/password.cnf
# Or can be checked in any of the services config files:
# `cat /etc/nova/nova.conf | grep mysql`
db_host = <src_mysql_host>

# Port for mysql connection
db_port = 3306

# Driver for connection
db_connection = mysql+pymysql


#==============================================================================
# Source cloud RabbitMQ configuration
#==============================================================================
[src_rabbit]

# User for RabbitMQ.
# On Controller node:
# `cat /etc/nova/nova.conf | grep rabbit_user`
user = <rabbit_user>

# Password for RabbitMQ user.
# On Controller node:
# `cat /etc/nova/nova.conf | grep rabbit_pas`
password = <rabbit_password>

# RabbitMQ host or comma separated RabbitMQ hosts in case Cluster solution.
# To find IPs of Rabbit: `cat /etc/nova/nova.conf | grep rabbit_hosts`
hosts = <rabbit_hosts>


#==============================================================================
# Source cloud compute service (nova) configuration
#==============================================================================
[src_compute]

# Live migration type. Passed directly to `nova live-migration` command.
block_migration = True

# Used for live-migration allow disk overcommit. Passed directly to
# `nova live-migration` command.
disk_overcommit = False

# Driver for DB connection
db_connection = mysql+pymysql

# Compute mysql node ip address. Usually controller node.
# `cat /etc/nova/nova.conf | grep mysql`
db_host = <nova_db_host>

# Port for MySQL connection
db_port = 3306

# Compute service DB name. `nova` by default.
# `cat /etc/nova/nova.conf | grep connection`
db_name = nova

# Database user for the Compute service.
# `cat /etc/nova/nova.conf | grep connection`
db_user = <nova_db_user>

# Database user password for the Compute service.
# `cat /etc/nova/nova.conf | grep connection`
db_password = <nova_db_password>


#==============================================================================
# Source cloud storage service (cinder) configuration
#==============================================================================
[src_storage]

# Name service for storage. Reserved for future
service = cinder

# Backend for storage
# Values:
#  nfs - works with generic cinder NFS and NetApp NFS drivers
#  iscsi-vmax - works with iSCSI VMAX cinder backend
#  shared-nfs - re-attaching volumes from source
backend = nfs

# Cinder mysql node ip address.
# `cat /etc/cinder/cinder.conf | grep mysql`
db_host = <src_cinder_db_host>

# Port for mysql connection.
# `cat /etc/cinder/cinder.conf | grep mysql`
db_port = 3306

# Driver for DB connection
db_connection = mysql+pymysql

# Database user for the Cinder service.
# `cat /etc/cinder/cinder.conf | grep mysql`
db_user = <cinder_database_username>

# Database user password for the Cinder service. Usually controller node.
# To get the password value: `cat /etc/cinder/cinder.conf | grep mysql`
db_password = <cinder_database_password>

# Compute service database name.
# `cat /etc/cinder/cinder.conf | grep mysql`
db_name = <cinder_database_name>

# Format for converting volumes.
# Values:
# qcow2 - default
disk_format = qcow2

# NFS-specific
# template for volume file names, `grep volume_name_template cinder.conf`
# volume_name_template=volume-%s

# A list of NFS mount points
nfs_mount_point_bases = /var/lib/cinder

# iSCSI VMAX-specific options

# IP address or hostname of EMC VMAX storage.
# vmax_ip

# IP port of EMC VMAX storage
# vmax_port

# Username to access EMC VMAX APIs.
# vmax_user

# Pasword required to access EMC VMAX APIs
# vmax_password

# Port group used for EMC VMAX
# vmax_port_groups

# Fast policy for EMC VMAX
# vmax_fast_policy

# Pool name for EMC VMAX
# vmax_pool_name

# Local host IP address which is used to connect to iSCSI target
# iscsi_my_ip

# InitiatorName from /etc/iscsi/initiatorname.iscsi
# initiator_name


#==============================================================================
# Source cloud image service (glance) configuration
#==============================================================================
[src_image]

# Name service for image based storage
service = glance

# Backend for images
# Values:
# swift -
# file - default
backend = swift

# Glance DB configuration
db_host = <glance_db_host>
db_port = 3306
db_connection = mysql+pymysql

# Database user for Glance service.
# `cat /etc/glance/glance-api.conf | grep mysql`
db_user = <glance_db_user>

# Database user password for the Glance service.
# To get value run: #`cat /etc/glance/glance-api.conf | grep mysql`
db_password = <glance_db_password>

# Glance service database name. 
# To get value run: #`cat /etc/glance/glance-api.conf | grep mysql`
db_name = glance


#==============================================================================
# Source cloud identity service (keystone) configuration
#==============================================================================
[src_identity]

# Service name for Identity service.
service = keystone

# Keystone DB configuration options. If not set uses `[src_mysql]` values
db_name = keystone
db_user = <keystone_db_user>
db_password = <keystone_db_password>
db_host = <keystone_db_host>
db_port = 3306
db_connection = mysql+pymysql


#==============================================================================
# Source cloud networking service (quantum/neutron) configuration
#==============================================================================
[src_network]
# Name service for network. Value: "auto" which detects available service.
service = auto

# neutron DB configuration options. If not set uses `[src_mysql]` values
# Driver for DB connection
db_connection = mysql+pymysql

# IP address of Network service's DB node. # cat /etc/neutron/neutron.conf | grep mysql
db_host = <neutron_db_host>

# Port for DB connection. # cat /etc/neutron/neutron.conf | grep mysql
db_port = 3306

# Network service's database name. # cat /etc/neutron/neutron.conf | grep mysql
db_name = neutron

# Database user for the Network service. # cat /etc/neutron/neutron.conf | grep mysql
db_user = <neutron_db_user>

# Database user's password for the Network service. # cat /etc/neutron/neutron.conf | grep mysql
db_password = <neutron_db_password>

# Get all Neutron network quotas.
# Default value is False.
# Values:
#   False - Get only custom Neutron quotas;
#   True - Get all Neutron quotas (default and custom)
get_all_quota = False


#==============================================================================
# Source cloud object storage service (swift) configuration
#==============================================================================
[src_objstorage]

# Service name for object storage.
service = swift


#==============================================================================
# Destination cloud configuration options. See [src] for description of each
# config option
#==============================================================================
[dst]
type = os
auth_url = <src_auth_url>
host = <dst_api_host>
ssh_host = <dst_ssh_host>
ssh_user = <dst_ssh_user>
ssh_sudo_password = <dst_ssh_sudo_password>
# TODO: consider using bridge name instead
ext_cidr = <dst_external_net_cidr>
user = admin
password = admin
tenant = admin
region = myregion
temp = /root/merge
service_tenant = service
cacert = <sertificate_file>
insecure = False


[dst_mysql]
db_user = root
db_password =
db_host = <dst_mysql_host>
db_port = 3306
db_connection = mysql+pymysql


[dst_rabbit]
user = <rabbit_user>
password = <rabbit_password>
hosts = <rabbit_hosts>


[dst_compute]
service = nova
backend = iscsi
host_eph_drv = <dst_host_epehem_drv>
cpu_allocation_ratio = 16
ram_allocation_ratio = 1
disk_allocation_ratio = 0.9
block_migration = True
disk_overcommit = False
db_connection = mysql+pymysql
db_host = <nova_db_host>
db_port = 3306
db_name = nova
db_user = <nova_db_user>
db_password = <nova_db_password>


[dst_storage]
service = cinder

# Backend for storage
# Values:
#  nfs - works with generic cinder NFS and NetApp NFS drivers
#  iscsi-vmax - works with iSCSI VMAX cinder backend
#  shared-nfs - re-attaching volumes from source
backend = nfs

db_host = <dst_cinder_db_host>
db_port = 3306
db_connection = mysql+pymysql
db_user = <cinder_database_username>
db_password = <cinder_database_password>
db_name = <cinder_database_name>

# NFS
# volume_name_template=volume-%s
# nfs_mount_point_bases = /var/lib/cinder

# iSCSI VMAX
# vmax_ip
# vmax_port
# vmax_user
# vmax_password
# vmax_port_groups
# vmax_fast_policy
# vmax_pool_name
# iscsi_my_ip
# initiator_name


[dst_image]
service = glance
backend = swift
db_host = <glance_db_host>
db_port = 3306
db_connection = mysql+pymysql
db_user = <glance_db_user>
db_password = <glance_db_password>
db_name = glance


[dst_identity]
service = keystone


[dst_network]
service = auto
db_connection = mysql+pymysql
db_host = <neutron_db_host>
db_port = 3306
db_name = neutron
db_user = <neutron_db_user>
db_password = <neutron_db_password>
get_all_quota = False


[dst_objstorage]
service = swift


[import_rules]
key = {name:dest-key-1}


[initial_check]
claimed_bandwidth = 100
factor = 0.5
test_file_size = 1024


[condense]
group_file=


[database]

#Redis database host location. Usually same as Cloudferry installed or separate Redis DB.
host = localhost

#Redis database port number.
port = 6379

[evacuation]
# Home directory of user under which nova services are running
nova_home_path = /var/lib/nova
# Name of user under which nova services are running
nova_user = nova
# For how much seconds to wait for VM state change during evacuation
state_change_timeout = 120
# For how much seconds to wait for VM to migrate during evacuation
migration_timeout = 600

[bbcp]
# path to the executable bbcp to execute on the cloudferry host
path = <bbcp_path>
# path to the compiled bbcp for the source cloud hosts
src_path = $path
# path to the compiled bbcp for the destination cloud hosts
dst_path = $path
# additional options
options = -P 5 -e

[rsync]
# The port of a tunnel to destination host. Used in case
# source and destination hosts have no direct connectivity
# (direct_transfer = False).
# In case of parallel execution on same controller
# the ports must be different.
port = 50000

[rollback]
# Don not remove successfully migrated vms from dst and don not
# restore the  statuses of those vms on src at rollback. (boolean
# value)
keep_migrated_vms = True


[mysqldump]
# Run mysqldump command on localhost where CloudFerry is running.
run_mysqldump_locally = True

# Allows to specify host for executing mysqldump command.
# Useful when MySQL uses multinode configuration with load balancer in front.
# In this case one would set VIP hostname in `db_host` and specific
# SSH-accessible MySQL node behind VIP for executing mysqldump command.
mysqldump_host = <specific MySQL DB hostname or IP address, not VIP>

# Filename template to store a dump of a database.
db_dump_filename = dump_{position}_{database}_{time}.sql

# Make a snapshot of all databases for source and destination clouds
dump_all_databases = False
